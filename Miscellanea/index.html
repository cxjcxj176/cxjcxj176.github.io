<!DOCTYPE html>
<html lang="en">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Xiaojiang Cheng


</title>
<meta name="description" content="Xiaojiang Cheng's Homepage.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ“¸</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https:/katjaschwarz.github.io/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/Research/">
                Research
                
              </a>
          </li>

           <li class="nav-item ">
              <a class="nav-link" href="/Teaching/">
                Teaching
                
              </a>
          </li>
          
           <li class="nav-item ">
              <a class="nav-link" href="/Miscellanea/">
                Miscellanea
                
              </a>
          </li>
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Xiaojiang Cheng
    </h1>

  </header>

  <article>
    
    <div class="profile float-right">
      
        <picture>
    
    <source media="(max-width: 480px)" srcset="/assets/resized/prof_pic-480x567.png">
    
    <source media="(max-width: 800px)" srcset="/assets/resized/prof_pic-800x945.png">
    
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.png" alt="prof_pic.png">
</source></source></picture>

      
      
        <div class="address">
          
        </div>
      
    </div>
    

    <div class="clearfix">
      <p>I am a PhD student in the <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/home/" target="_blank" rel="noopener noreferrer">Autonomous Vision Group (AVG)</a> at <a href="https://uni-tuebingen.de/en/" target="_blank" rel="noopener noreferrer">TÃ¼bingen University</a> and the <a href="https://is.mpg.de/" target="_blank" rel="noopener noreferrer">Max Planck Institute for Intelligent Systems</a>, advised by <a href="http://cvlibs.net/" target="_blank" rel="noopener noreferrer">Andreas Geiger</a>.</p>

<p>My research lies at the intersection of computer vision and graphics and focuses on 3D vision. In particular, I am interested in enabeling machines to infer 3D representations from sparse observations, such as 2D images. I am passionate about leveraging such representations for generative modeling in 2D and 3D.</p>

<p>I studied Physics at <a href="https://www.uni-heidelberg.de/en" target="_blank" rel="noopener noreferrer">Heidelberg University</a> where I received my bachelor degree in 2016 and master degree in 2018. 
In July 2019, I started my PhD in computer vision / machine learning in the <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/home/" target="_blank" rel="noopener noreferrer">Autonomous Vision Group (AVG)</a> at <a href="https://uni-tuebingen.de/en/" target="_blank" rel="noopener noreferrer">TÃ¼bingen University</a> and the <a href="https://is.mpg.de/" target="_blank" rel="noopener noreferrer">Max Planck Institute for Intelligent Systems</a> in TÃ¼bingen, Germany, 
under the supervision of <a href="http://cvlibs.net/" target="_blank" rel="noopener noreferrer">Andreas Geiger</a>.</p>


      
      <div class="social">
        <div class="contact-icons">
          <a href="mailto:%6B%61%74%6A%61.%73%63%68%77%61%72%7A@%75%6E%69-%74%75%65%62%69%6E%67%65%6E.%64%65" title="email"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=mhOrpHIAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/katjaschwarz" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/katja-schwarz" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/K_S_Schwarz" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>













        </div>
        <div class="contact-note"></div>
      </div>
      
    </div>

    
      <div class="news">
  <h2>news</h2>

        Our <a href="assets/pdf/1.pdf" target="_blank" rel="noopener noreferrer">PDF</a>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless" style="width: 100%">
        <colgroup>
           <col span="1" style="width: 15%;">
           <col span="1" style="width: 85%;">
        </colgroup>
      <!-- Put <thead>, <tbody>, and <tr>'s here! -->
      <tbody>
        
        
          <tr>
            <th scope="row">Jan 7, 2023</th>
            <td>
              
                I gave a talk about 3D-aware image synthesis at the WACV workshop on <a href="https://sites.google.com/view/piesworkshop/pies-cv2023" target="_blank" rel="noopener noreferrer">Photorealistic Image and Environment Synthesis 
for Computer Vision</a>

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jul 3, 2022</th>
            <td>
              
                Our <a href="https://arxiv.org/pdf/2202.00273.pdf" target="_blank" rel="noopener noreferrer">ARAH</a> paper was accepted at ECCVâ€™22!

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jul 1, 2022</th>
            <td>
              
                I am excited to join the <a href="https://nv-tlabs.github.io/" target="_blank" rel="noopener noreferrer">Toronto AI Lab</a> at Nvidia for an internship this summer.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Mar 27, 2022</th>
            <td>
              
                Our <a href="https://arxiv.org/pdf/2202.00273.pdf" target="_blank" rel="noopener noreferrer">StyleGAN-XL</a> paper was accepted at SIGGRAPHâ€™22!

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Dec 2, 2021</th>
            <td>
              
                Our <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf" target="_blank" rel="noopener noreferrer">GRAF paper</a> won the award for the most promising scientific performance at <a href="https://www.aigame.dev/" target="_blank" rel="noopener noreferrer">AIGameDev</a>

              
            </td>
          </tr>
        
      </tbody>
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/voxgraf.gif" alt="VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids" style="width: 100%;">
    </div>
    
  </div>

  <div id="Schwarz2022NEURIPS" class="col-md-9">
    
      <div class="title">VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Schwarz, Katja</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://axelsauer.com/" target="_blank" rel="noopener noreferrer">Sauer, Axel</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://m-niemeyer.github.io/" target="_blank" rel="noopener noreferrer">Niemeyer, Michael</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://yiyiliao.github.io/" target="_blank" rel="noopener noreferrer">Liao, Yiyi</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://www.cvlibs.net/publications/Schwarz2022NEURIPS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
      <a href="https://www.machinelearningforscience.de/en/escaping-platos-cave-teaching-machines-the-3d-nature-of-our-world" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://github.com/autonomousvision/voxgraf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://katjaschwarz.github.io/voxgraf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to parameterize 3D radiance fields. While demonstrating impressive results, querying an MLP for every sample along each ray leads to slow rendering. Therefore, existing approaches often render low-resolution feature maps and process them with an upsampling network to obtain the final image. Albeit efficient, neural rendering often entangles viewpoint and content such that changing the camera pose results in unwanted changes of geometry or appearance. Motivated by recent results in voxel-based novel view synthesis, we investigate the utility of sparse voxel grid representations for fast and 3D-consistent generative modeling in this paper. Our results demonstrate that monolithic MLPs can indeed be replaced by 3D convolutions when combining sparse voxel grids with progressive growing, free space pruning and appropriate regularization. To obtain a compact representation of the scene and allow for scaling to higher voxel resolutions, our model disentangles the foreground object (modeled in 3D) from the background (modeled in 2D). In contrast to existing approaches, our method requires only a single forward pass to generate a full 3D scene. It hence allows for efficient rendering from arbitrary viewpoints while yielding 3D consistent results with high visual fidelity.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/styleganxl.gif" alt="StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets" style="width: 100%;">
    </div>
    
  </div>

  <div id="Sauer2022SIGGRAPH" class="col-md-9">
    
      <div class="title">StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://axelsauer.com/" target="_blank" rel="noopener noreferrer">Sauer, Axel</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Schwarz, Katja</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ACM Trans. on Graphics</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://www.cvlibs.net/publications/Sauer2022SIGGRAPH.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="https://www.cvlibs.net/publications/Sauer2022SIGGRAPH_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
    
      <a href="https://github.com/autonomousvision/stylegan-xl" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://sites.google.com/view/stylegan-xl/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    <a href="https://www.youtube.com/watch?v=c-PjRXVG8ZI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Computer graphics has experienced a recent surge of data-centric approaches for photorealistic and controllable content creation. StyleGAN in particular sets new standards for generative modeling regarding image quality and controllability. However, StyleGANâ€™s performance severely degrades on large unstructured datasets such as ImageNet. StyleGAN was designed for controllability; hence, prior works suspect its restrictive design to be unsuitable for diverse datasets. In contrast, we find the main limiting factor to be the current training strategy. Following the recently introduced Projected GAN paradigm, we leverage powerful neural network priors and a progressive growing strategy to successfully train the latest StyleGAN3 generator on ImageNet. Our final model, StyleGAN-XL, sets a new state-of-the-art on large-scale image synthesis and is the first to generate images at a resolution of 1024x1024 at such a dataset scale. We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific objectÂ classes.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/frequency_bias.gif" alt="On the Frequency Bias of Generative Models" style="width: 100%;">
    </div>
    
  </div>

  <div id="Schwarz2021NEURIPS" class="col-md-9">
    
      <div class="title">On the Frequency Bias of Generative Models</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Schwarz, Katja</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://yiyiliao.github.io/" target="_blank" rel="noopener noreferrer">Liao, Yiyi</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2021NEURIPS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2021NEURIPS_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/frequency-bias/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://github.com/autonomousvision/frequency_bias" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2021NEURIPS_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
    
    <a href="https://www.youtube.com/watch?v=jbcB-hHoOIA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The key objective of Generative Adversarial Networks (GANs) is to generate new data
  with the same statistics as the provided training data. However, multiple recent works show that
  state-of-the-art architectures yet struggle to achieve this goal. In particular, they report an
  elevated amount of high frequencies in the spectral statistics which makes it straightforward to
  distinguish real and generated images. Explanations for this phenomenon are controversial: While
  most works attribute the artifacts to the generator, other works point to the discriminator. We
  take a sober look at those explanations and provide insights on what makes proposed measures
  against high-frequency artifacts effective. To achieve this, we first independently assess the
  architectures of both the generator and discriminator and investigate if they exhibit a frequency
  bias that makes learning the distribution of high-frequency content particularly problematic.
  Based on these experiments, we make the following four observations: 1) Different upsampling
  operations bias the generator towards different spectral properties. 2) Checkerboard artifacts
  introduced by upsampling cannot explain the spectral discrepancies alone as the generator is able
  to compensate for these artifacts. 3) The discriminator does not struggle with detecting high
  frequencies per se but rather struggles with frequencies of low magnitude. 4) The downsampling
  operations in the discriminator can impair the quality of the training signal it provides. In
  light of these findings, we analyze proposed measures against high-frequency artifacts in
  state-of-the-art GAN training but find that none of the existing approaches can fully resolve
  spectral artifacts yet. Our results suggest that there is great potential in improving the
  discriminator and that this could be key to match the distribution of the training data more
  closely.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/graf.gif" alt="GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis" style="width: 100%;">
    </div>
    
  </div>

  <div id="Schwarz2020NEURIPS" class="col-md-9">
    
      <div class="title">GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Schwarz, Katja</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://yiyiliao.github.io/" target="_blank" rel="noopener noreferrer">Liao, Yiyi</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://m-niemeyer.github.io/" target="_blank" rel="noopener noreferrer">Niemeyer, Michael</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/graf/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://github.com/autonomousvision/graf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
    
    <a href="https://www.youtube.com/watch?v=akQf7WaCOHo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While 2D generative adversarial networks have enabled high-resolution image synthesis,
  they largely lack an understanding of the 3D world and the image formation process. Thus, they do
  not provide precise control over camera viewpoint or object pose. To address this problem, several
  recent approaches leverage intermediate voxel-based representations in combination with differentiable
  rendering. However, existing methods either produce low image resolution or fall short in disentangling
  camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper,
  we propose a generative model for radiance fields which have recently proven successful for novel view
  synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not
  confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene
  properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a
  multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training
  our model from unposed 2D images alone. We systematically analyze our approach on several challenging
  synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful
  representation for generative image synthesis, leading to 3D consistent models that render with high
  fidelity.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>
</div>

    

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    Â© Copyright 2023 Katja  Schwarz.
    Created using the <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme.
    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
